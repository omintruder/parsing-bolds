{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErariXWZ3Si6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5715347d-3064-4d34-ce9d-e6f343050b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the function that parses words in bold\n",
        "\n",
        "///\n",
        "\n",
        "функция, парсящая выделеннные жирным слова"
      ],
      "metadata": {
        "id": "r59M_qKvX9nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_bold_text(document):\n",
        "  \"\"\"\n",
        "  extracts bold text from a DOCX document.\n",
        "  :param input: a docx file with kev's text\n",
        "  :return: a list of bold text strings found in the document, normalized\n",
        "  \"\"\"\n",
        "  bold_text = []\n",
        "  for paragraph in document.paragraphs:\n",
        "    for word in paragraph.runs:\n",
        "      if word.bold:\n",
        "        bold_text.append(word.text)\n",
        "  return bold_text\n"
      ],
      "metadata": {
        "id": "7T95Nnl-4ArG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalization: deleting duples, capilazing\n",
        "\n",
        "///\n",
        "\n",
        "\n",
        "нормализация: удаление дубликатов, капитализация"
      ],
      "metadata": {
        "id": "HQHfRusCYGs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_normalizing(input):\n",
        "  \"\"\"\n",
        "  deletes duplicates and capitilizes words\n",
        "  :param input: '1', '0,', 'abggah', 'aur', '1', 'twuht', 'aur', 'TwuhT'\n",
        "  :return: '1', '0,', 'Abggah', 'Aur', 'Twuht'\n",
        "  \"\"\"\n",
        "  result = []\n",
        "  for elem in input:\n",
        "    if elem.capitalize() not in result:\n",
        "      result.append(elem.capitalize())\n",
        "  return result"
      ],
      "metadata": {
        "id": "sIwjZOi1mHoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing and enacting functions. argument of Document is, in this case of Google Collab, the address of the parsed file in Google Drive. It can be copied in rightclick.\n",
        "\n",
        "///\n",
        "\n",
        "парсинг и применение функций. аргумент Document() -- адрес файла в гугл драйве. его можно скопировать райткликом."
      ],
      "metadata": {
        "id": "rdxfbbUiYZYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/kevin\\'s texts.docx' #любое имя фай\n",
        "document = Document(file_path)\n",
        "bold_words = get_bold_text(document)\n",
        "normalized_bolds = final_normalizing([word for word in bold_words if word.isalpha()])\n",
        "words_count = len(normalized_bolds)"
      ],
      "metadata": {
        "id": "oC2rObpx_JZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing a final result, enumerated\n",
        "\n",
        "///\n",
        "\n",
        "печать пронумерованного конечного результата"
      ],
      "metadata": {
        "id": "5oQva3k8YdRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, word in enumerate(normalized_bolds, 1):\n",
        "  print(f'{index}. {word}')\n",
        "print(f'Here are {words_count} parsed words.')"
      ],
      "metadata": {
        "id": "ZP_gdp79_F2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7dedebf-2d1b-40fa-b544-851a7cf2ce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Tainted\n",
            "2. Distinguished\n",
            "3. Delectable\n",
            "4. Dwindled\n",
            "5. Harvest\n",
            "6. Extinction\n",
            "7. Bribery\n",
            "8. Enticed\n",
            "9. Overlook\n",
            "10. Mask\n",
            "11. Advocate\n",
            "12. Trafficking\n",
            "13. Intractable\n",
            "14. Capture\n",
            "15. Surreptitiously\n",
            "16. Perpetual\n",
            "17. Contemporary\n",
            "18. Tailored\n",
            "19. Cunning\n",
            "20. Crutch\n",
            "21. Inherent\n",
            "22. Rationalize\n",
            "23. Incredulous\n",
            "24. Plausible\n",
            "25. Coffin\n",
            "26. Stitched\n",
            "27. Appalling\n",
            "28. Eulogized\n",
            "29. Dreadful\n",
            "30. Deteriorate\n",
            "31. Negligence\n",
            "32. Grave\n",
            "33. Verdict\n",
            "34. Enforce\n",
            "35. Compelled\n",
            "36. Undignified\n",
            "37. Starve\n",
            "38. Fuss\n",
            "39. Dubbed\n",
            "40. Sue\n",
            "41. Objectionable\n",
            "42. Shirk\n",
            "43. Mediate\n",
            "44. Disgrace\n",
            "45. Immeasurably\n",
            "46. Amicable\n",
            "47. Recalcitrant\n",
            "48. Predicted\n",
            "49. Transcendence\n",
            "50. Centerpiece\n",
            "51. Promote\n",
            "52. Striving\n",
            "53. Gap\n",
            "54. Vastly\n",
            "55. Ranked\n",
            "56. Fulfilled\n",
            "57. Routine\n",
            "58. Delusions\n",
            "59. Evolved\n",
            "60. Mismatch\n",
            "61. Intangible\n",
            "62. Amplified\n",
            "63. Traces\n",
            "64. Urge\n",
            "65. Junkies\n",
            "66. Blueprint\n",
            "67. Exhaustion\n",
            "68. Subsidized\n",
            "69. Devotion\n",
            "70. Spare\n",
            "71. Grips\n",
            "72. Secular\n",
            "73. Perspective\n",
            "74. Inflict\n",
            "75. Anonymity\n",
            "76. Bleak\n",
            "77. Shadowing\n",
            "78. Rollicking\n",
            "79. Miserable\n",
            "80. Bragged\n",
            "81. Underlings\n",
            "82. Limbs\n",
            "83. Sheer\n",
            "84. Enhance\n",
            "85. Suppressed\n",
            "86. Irreversible\n",
            "87. Wildly\n",
            "88. Stretches\n",
            "89. Splinter\n",
            "90. Psyches\n",
            "91. Bulletproof\n",
            "92. Regained\n",
            "93. Tempt\n",
            "94. Rhapsodies\n",
            "95. Intrinsically\n",
            "96. Skews\n",
            "97. Derive\n",
            "98. Exact\n",
            "99. Intermediaries\n",
            "100. Stigma\n",
            "101. Camouflaging\n",
            "Here are 101 parsed words.\n"
          ]
        }
      ]
    }
  ]
}
